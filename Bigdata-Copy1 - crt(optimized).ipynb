{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rake_nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rake_nltk) (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk->rake_nltk) (1.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "#Aspect extraction part\n",
    "!pip install rake_nltk\n",
    "from rake_nltk import Rake\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords  \n",
    "from pywsd.utils import lemmatize_sentence\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"mode.chained_assignment\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel(r\"E:\\0001_Studies\\RnD-CustomerBehaviourAnalytics\\RnD Survey Data Actual.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sl. No', 'Year', 'Study Name/Product Code', 'Type of study',\n",
       "       'Type of product', 'Name', 'Gender', 'Region', 'Likes', 'Dislikes',\n",
       "       ...\n",
       "       'CR Cool feel', 'CR Deep cleaning from head to toe',\n",
       "       'CR Energizing your body', 'PI', 'AS Test product',\n",
       "       'AS Regular use product', 'AP Liked purchased',\n",
       "       'AP Liked Not purchased', 'AP Disliked purchased',\n",
       "       'AP Disliked Not Purchased'],\n",
       "      dtype='object', length=141)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Foundation Talc - ( 541)               93\n",
       " Fairness Serum - 419.                 70\n",
       "Spinz winter Cream                     69\n",
       " Fairness  Cream - (734)               65\n",
       "Nyle Winter Facial Cleanser            65\n",
       "Fairness Cream - ( 381)                55\n",
       "Spinz BB Fairness Cream                37\n",
       "NYLE BOUTIQUE MOISTURIZING LOTION      35\n",
       "NB - Moisturizing Lotion               35\n",
       "Face Pack - 352                        33\n",
       "Biker's Men Anti-Pollution Facewash    31\n",
       "Nyle Boutique Brightening cream        30\n",
       "Biker's Men Shower Gel                 29\n",
       "Biker's Men Oil Control Facewash       29\n",
       "Biker's Men's Face Cream               18\n",
       "Improved Fairness Cream                15\n",
       "Name: Study Name/Product Code, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Study Name/Product Code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "str=['softness','brightening','shiny','skin','fragrance','whitening','smoothness','aroma','spreading','season','color','cream','brightness','soft','smooth','long','fairness','non oily','bright','soft','smooth','smoothy','lasting','thickness','mild','smell','dryness','pacth','brighten','brightlook','look','freshness','fresh','yes','creamy','cleansing','moisturise','pimple','face','milky','fruit','smell','glow','olive','oil','healthy','clean','foam','quantity','purchase','repurchase','water','packaging','smudge','moisturing','pdt','sandal','quantity','talc','packing','powder','costly','sweat','control','absorb','rough','hair','tangle','dirt',\n",
    "'sticky','dryness','texture','dullness','oily','dust','moisturizing','frag','moisturizer','facial','soapy','hydrate','avacado','butter','spread','moisturize','natural','flavour','tightness','scrub','colour','whiteness','circle','dark','remove','result','rash','itch','tighten','granule','irritation','thin','effect','garnier','coolness','chemical','dry','thick','whity','moisture','pollution','blemish','alovera','spot','sunscreen','sweat','qty','patchy','shining','wrinkle','shine','spreadability','sweating','pack','brand','price','pdt','market','fairness','time','nutrition','clean','moisturizer','blackness','oilyness','tanning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Likes'].replace('\\d+', \"\", regex=True,inplace= True )\n",
    "data['Likes'] = data['Likes'].str.replace(\"/\", \" and \")\n",
    "data['Likes']= data['Likes'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "data['Likes'] = data['Likes'].replace(np.nan, \" \", regex=True)\n",
    "data['Likess'] = data['Likes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "places=['dl']\n",
    "#xtracting loc and location \n",
    "pattern='|'.join(places)\n",
    "x=data['Likes'].str.contains(pattern)\n",
    "data['Sentiment']=x.replace((True,False),('negative','positive'))\n",
    "data['Likes'] = data['Likes'].str.replace(\"dl\", \" \")\n",
    "data['Likes']=[x for x in data['Likes'].str.lower().str.split()]\n",
    "data['Liked_Aspect']=data['Likes'].apply(lambda x: [x for x in x if x in str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting aspect with higher ranking phrases (top5 min)\n",
    "aspect_adj_string =[]\n",
    "for i in data['Likes'].values:\n",
    "    try:\n",
    "        r= Rake()\n",
    "        r = Rake(min_length=1, max_length=2)\n",
    "        a=r.extract_keywords_from_text(i)\n",
    "        b=r.get_ranked_phrases()[:5]  \n",
    "        aspect_adj_string.append(b)\n",
    "    except:\n",
    "        aspect_adj_string.append(0)\n",
    "data['aspect_adj_string']=aspect_adj_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AP Liked purchased'] = data['AP Liked purchased'].replace(\"/\", \" and \")\n",
    "data['AP Liked purchased'] = data['AP Liked purchased'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "data['AP Liked purchased'] = data['AP Liked purchased'].replace(np.nan, '', regex=True)\n",
    "data['AP Liked purchased']=data['AP Liked purchased'].str.lower().str.split()\n",
    "data['AP_Likedpurchased_Aspect']=data['AP Liked purchased'].apply(lambda x: [x for x in x if x in str])\n",
    "data.drop(columns=['Likess'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AP Liked Not purchased'] = data['AP Liked Not purchased'].replace(\"/\", \" and \")\n",
    "data['AP Liked Not purchased']= data['AP Liked Not purchased'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "data['AP Liked Not purchased'] = data['AP Liked Not purchased'].replace(np.nan, '', regex=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data['AP Liked Not purchased'] = data['AP Liked Not purchased'].apply(lemmatize_sentence)\n",
    "data['AP_LikedNotpurchased_Aspect']=data['AP Liked Not purchased'].apply(lambda x: [x for x in x if x in str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AP Disliked purchased'] = data['AP Disliked purchased'].replace(\"/\", \" and \")\n",
    "data['AP Disliked purchased']= data['AP Disliked purchased'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "data['AP Disliked purchased'] = data['AP Disliked purchased'].replace(np.nan, '', regex=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data['AP Disliked purchased'] = data['AP Disliked purchased'].apply(lemmatize_sentence)\n",
    "data['AP_Dislikedpurchased_Aspect']=data['AP Disliked purchased'].apply(lambda x: [x for x in x if x in str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AP Disliked Not Purchased'] = data['AP Disliked Not Purchased'].replace(\"/\", \" and \")\n",
    "data['AP Disliked Not Purchased']= data['AP Disliked Not Purchased'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "data['AP Disliked Not Purchased'] = data['AP Disliked Not Purchased'].replace(np.nan, '', regex=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data['AP Disliked Not Purchased'] = data['AP Disliked Not Purchased'].apply(lemmatize_sentence)\n",
    "data['AP_Dislikednotpurchased_Aspect']=data['AP Disliked Not Purchased'].apply(lambda x: [x for x in x if x in str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Dislikes'] = data['Dislikes'].replace(\"/\", \" and \")\n",
    "data['Dislikes']= data['Dislikes'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "data['Dislikes'] = data['Dislikes'].replace(np.nan, '', regex=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data['Dislikes'] = data['Dislikes'].apply(lemmatize_sentence)\n",
    "data['Disliked_Aspect']=data['Dislikes'].apply(lambda x: [x for x in x if x in str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AS Test product'] = data['AS Test product'].replace(\"/\", \" and \")\n",
    "data['AS Test product']= data['AS Test product'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "data['AS Test product'] = data['AS Test product'].replace(np.nan, '', regex=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data['AS Test product'] = data['AS Test product'].apply(lemmatize_sentence)\n",
    "data['AS_Testproduct_Aspect']=data['AS Test product'].apply(lambda x: [x for x in x if x in str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Columns are\n",
    "\n",
    "#### 1. 'AP_Likedpurchased_Aspect'\n",
    "#### 2. 'AP_Likednotpurchased_Aspect'\n",
    "#### 3. 'AP_Dislikedpurchased_Aspect'\n",
    "#### 4. 'AP_Dislikednotpurchased_Aspect'\n",
    "#### 5. 'Disliked_Aspect' and 'Liked_Aspect'\n",
    "#### 6. 'AS_Testproduct_Aspect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Improved Fairness Cream ', ' Fairness Serum - 419.',\n",
       "       ' Fairness  Cream - (734)', 'Fairness Cream - ( 381)',\n",
       "       'Face Pack - 352', 'Foundation Talc - ( 541)',\n",
       "       'Spinz BB Fairness Cream', 'NB - Moisturizing Lotion',\n",
       "       'Nyle Winter Facial Cleanser',\n",
       "       \"Biker's Men Anti-Pollution Facewash\",\n",
       "       \"Biker's Men Oil Control Facewash\", \"Biker's Men Shower Gel\",\n",
       "       \"Biker's Men's Face Cream\", 'NYLE BOUTIQUE MOISTURIZING LOTION',\n",
       "       'Spinz winter Cream ', 'Nyle Boutique Brightening cream'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Product = data[\"Study Name/Product Code\"].unique()\n",
    "Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    "def get_frequncy(df_1,Product_list):\n",
    "    final_df = pd.DataFrame()    \n",
    "    \n",
    "    for i in Product_list:\n",
    "        \n",
    "        df = df_1[df_1['Study Name/Product Code']==i]\n",
    "        #print('Product is',i)\n",
    "        like_aspect_list = df['Liked_Aspect']\n",
    "        dislike_aspect_list = df['Disliked_Aspect']\n",
    "        Likedpurchased_aspect_list = df['AP_Likedpurchased_Aspect']\n",
    "        LikedNotpurchased_aspect_list = df['AP_LikedNotpurchased_Aspect']\n",
    "        Dislikedpurchased_aspect_list = df['AP_Dislikedpurchased_Aspect']\n",
    "        Dislikednotpurchased_aspect_list = df['AP_Dislikednotpurchased_Aspect']\n",
    "        Testproduct_aspect_list = df['AS_Testproduct_Aspect']\n",
    "\n",
    "        like_aspect_list = [item for items in like_aspect_list for item in items]\n",
    "        dislike_aspect_list = [item for items in dislike_aspect_list for item in items]\n",
    "        Likedpurchased_aspect_list = [item for items in Likedpurchased_aspect_list for item in items]\n",
    "        Likednotpurchased_aspect_list = [item for items in LikedNotpurchased_aspect_list for item in items]\n",
    "        Dislikedpurchased_aspect_list = [item for items in Dislikedpurchased_aspect_list for item in items]\n",
    "        Dislikednotpurchased_aspect_list = [item for items in Dislikednotpurchased_aspect_list for item in items]\n",
    "        Testproduct_aspect_list = [item for items in Testproduct_aspect_list for item in items]\n",
    "\n",
    "        like_aspect_count = Counter(like_aspect_list).items()\n",
    "        dislike_aspect_count = Counter(dislike_aspect_list).items()\n",
    "        Likedpurchased_aspect_count = Counter(Likedpurchased_aspect_list).items()\n",
    "        Likednotpurchased_aspect_count = Counter(Likednotpurchased_aspect_list).items()\n",
    "        Dislikedpurchased_aspect_count = Counter(Dislikedpurchased_aspect_list).items()\n",
    "        Dislikednotpurchased_aspect_count = Counter(Dislikednotpurchased_aspect_list).items()\n",
    "        Testproduct_aspect_count = Counter(Testproduct_aspect_list).items()\n",
    "\n",
    "        #Check for empty rows in dfframe\n",
    "        def fill_empty_dfframe(aspect):\n",
    "            if(all(x == [] for x in aspect)==True):\n",
    "                value = [{'Empty','Value'}]\n",
    "            elif(all(x == [] for x in aspect)==False):\n",
    "                value = aspect\n",
    "            return value\n",
    "\n",
    "        like_aspect_count = fill_empty_dfframe(like_aspect_count)\n",
    "        dislike_aspect_count = fill_empty_dfframe(dislike_aspect_count)\n",
    "        Likedpurchased_aspect_count = fill_empty_dfframe(Likedpurchased_aspect_count)\n",
    "        Likednotpurchased_aspect_count = fill_empty_dfframe(Likednotpurchased_aspect_count)\n",
    "        Dislikedpurchased_aspect_count = fill_empty_dfframe(Dislikedpurchased_aspect_count)\n",
    "        Dislikednotpurchased_aspect_count = fill_empty_dfframe(Dislikednotpurchased_aspect_count)\n",
    "        Testproduct_aspect_count = fill_empty_dfframe(Testproduct_aspect_count)\n",
    "\n",
    "        Like_aspect_Frequency = pd.DataFrame.from_dict(like_aspect_count)\n",
    "        Like_aspect_Frequency.rename(columns={0:'Word',1:'Like_aspect_Frequency'},inplace=True)\n",
    "        dislike_aspect_Frequency = pd.DataFrame.from_dict(dislike_aspect_count)\n",
    "        dislike_aspect_Frequency.rename(columns={0:'Word',1:'Dislike_aspect_Frequency'},inplace=True)\n",
    "        Likedpurchased_Frequency = pd.DataFrame.from_dict(Likedpurchased_aspect_count)\n",
    "        Likedpurchased_Frequency.rename(columns={0:'Word',1:'Likedpurchased_Frequency'},inplace=True)\n",
    "        Likednotpurchased_Frequency = pd.DataFrame.from_dict(Likednotpurchased_aspect_count)\n",
    "        Likednotpurchased_Frequency.rename(columns={0:'Word',1:'Likednotpurchased_Frequency'},inplace=True)\n",
    "        Dislikedpurchased_Frequency = pd.DataFrame.from_dict(Dislikedpurchased_aspect_count)\n",
    "        Dislikedpurchased_Frequency.rename(columns={0:'Word',1:'Dislikedpurchased_Frequency'},inplace=True)\n",
    "        Dislikednotpurchased_Frequency = pd.DataFrame.from_dict(Dislikednotpurchased_aspect_count)\n",
    "        Dislikednotpurchased_Frequency.rename(columns={0:'Word',1:'Dislikednotpurchased_Frequency'},inplace=True)\n",
    "        Testproduct_Frequency = pd.DataFrame.from_dict(Testproduct_aspect_count)\n",
    "        Testproduct_Frequency.rename(columns={0:'Word',1:'Testproduct_Frequency'},inplace=True)\n",
    "\n",
    "        df_frames = [Like_aspect_Frequency,dislike_aspect_Frequency,Likedpurchased_Frequency,Dislikedpurchased_Frequency,\n",
    "                    Dislikednotpurchased_Frequency,Testproduct_Frequency,Likednotpurchased_Frequency]\n",
    "        Aspect_df = reduce(lambda  left,right: pd.merge(left,right,on=['Word'],how='outer'), df_frames)\n",
    "\n",
    "        Aspect_df[\"Study Name/Product Code\"] = np.repeat(i,len(Aspect_df))\n",
    "        final_df = final_df.append(Aspect_df)\n",
    "    final_df.fillna(0,inplace=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = get_frequncy(data,Product)\n",
    "#final_output.sample(10)\n",
    "    final_output.to_excel(r\"E:\\0001_Studies\\RnD-CustomerBehaviourAnalytics\\output\\final_NLP_output.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
